{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../config.env\")\n",
    "os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in IDSR.txt\n",
    "with open(\"IDSR.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d72066",
   "metadata": {},
   "source": [
    "Extract Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a helpful assistant. Extract a list of 30–50 key symptoms, signs, or diagnostic terms from the following disease descriptions.\n",
    "\n",
    "Focus on words or phrases that are likely to appear in clinical case definitions or user queries — such as \"fever\", \"skin lesions\", \"swollen lymph nodes\", \"positive blood smear\", etc.\n",
    "\n",
    "Only return the keywords or short phrases — one per line.\n",
    "\n",
    "Text:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f704812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt + text}\n",
    "    ],\n",
    "    temperature=0.0\n",
    ")\n",
    "keywords = [line.strip() for line in response.choices[0].message.content.splitlines() if line.strip()]\n",
    "print(\"Extracted Keywords:\")\n",
    "for keyword in keywords:\n",
    "    print(\"-\", keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f698154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dashes and normalize keywords\n",
    "def normalize_kw(kw):\n",
    "    return kw.lstrip(\"-• \").strip().lower() \n",
    "keywords = [normalize_kw(kw) for kw in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11324098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save keywords to file\n",
    "with open(\"idsr_keywords.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for keyword in keywords:\n",
    "        f.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "with open(\"idsr_keywords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    keywords = [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12b253",
   "metadata": {},
   "source": [
    "Prep each disease as a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to split the text into a list of dictionaries:\n",
    "# the text is structured as follows:\n",
    "# the section for each disease starts after an empty line.\n",
    "# the disease name itself takes up the first line.\n",
    "# following the disease name, there will be subsections, each one beginning with an \"-\", some text, and then a colon. \n",
    "# what is between the \"-\" and the colon is the name of the subsection. the name of each subsection takes up one line.\n",
    "# following this, the next few lines contains the text for that subsection. however many lines it takes up,\n",
    "# this should be the value for the subsection key in the dictionary, condenses to a single string.\n",
    "# some diseases have multiple subsections, while others have only one.\n",
    "# when we encounter an empty line, it indicates the start of a new disease section.\n",
    "# what we should produce is one dictionary per disease, with a key called disease_name and value being the name of the disease. \n",
    "# the other keys should be the subsections, with the value being the text that follows the subsection name.\n",
    "\n",
    "def parse_disease_text(text):\n",
    "    diseases = []\n",
    "    lines = text.strip().splitlines()\n",
    "    \n",
    "    current_disease = None\n",
    "    current_subsection = None\n",
    "    buffer = []\n",
    "\n",
    "    def finalize_subsection():\n",
    "        if current_disease is not None and current_subsection and buffer:\n",
    "            content = \" \".join(line.strip() for line in buffer).strip()\n",
    "            current_disease[current_subsection] = content\n",
    "\n",
    "    subsection_pattern = re.compile(r\"^-\\s*(.+):\\s*$\")\n",
    "\n",
    "    for line in lines + [\"\"]:  # Extra empty line to trigger final save\n",
    "        if not line.strip():\n",
    "            finalize_subsection()\n",
    "            if current_disease:\n",
    "                diseases.append(current_disease)\n",
    "            current_disease = None\n",
    "            current_subsection = None\n",
    "            buffer = []\n",
    "            continue\n",
    "\n",
    "        if current_disease is None:\n",
    "            current_disease = {\"disease_name\": line.strip()}\n",
    "            continue\n",
    "\n",
    "        match = subsection_pattern.match(line)\n",
    "        if match:\n",
    "            finalize_subsection()\n",
    "            current_subsection = match.group(1).strip()\n",
    "            buffer = []\n",
    "        else:\n",
    "            buffer.append(line.rstrip())\n",
    "\n",
    "    return diseases\n",
    "\n",
    "\n",
    "\n",
    "disease_dicts = parse_disease_text(text)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd83b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def convert_disease_dicts_to_documents(disease_dicts):\n",
    "    docs = []\n",
    "    for disease in disease_dicts:\n",
    "        disease_name = disease.get(\"disease_name\", \"\")\n",
    "        subsections = [f\"{key}:\\n{value}\" for key, value in disease.items() if key != \"disease_name\"]\n",
    "        full_text = f\"Disease: {disease_name}\\n\\n\" + \"\\n\\n\".join(subsections)\n",
    "        docs.append(Document(page_content=full_text, metadata={\"disease_name\": disease_name}))\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19baadb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert to LangChain documents\n",
    "documents = convert_disease_dicts_to_documents(disease_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc8f40",
   "metadata": {},
   "source": [
    "Tag each document with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d70fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "def tag_documents_with_keywords(documents, keywords, threshold=85):\n",
    "    \"\"\"\n",
    "    Tags each Document in the list with a 'matched_keywords' metadata field\n",
    "    using fuzzy matching (e.g., RapidFuzz partial ratio).\n",
    "\n",
    "    Parameters:\n",
    "        documents (list): List of langchain `Document` objects.\n",
    "        keywords (list): List of predefined clinical keywords (e.g. from GPT).\n",
    "        threshold (int): Similarity threshold (0–100) for fuzzy matching.\n",
    "\n",
    "    Returns:\n",
    "        List of tagged Document objects with updated metadata.\n",
    "    \"\"\"\n",
    "    tagged = []\n",
    "\n",
    "    for doc in documents:\n",
    "        content = doc.page_content.lower()\n",
    "\n",
    "        # Match keywords against document content\n",
    "        matched = []\n",
    "        for kw in keywords:\n",
    "            kw_lower = kw.lower()\n",
    "            if fuzz.partial_ratio(kw_lower, content) >= threshold:\n",
    "                matched.append(kw)\n",
    "\n",
    "        # Add tags to metadata\n",
    "        doc.metadata[\"matched_keywords\"] = matched\n",
    "        tagged.append(doc)\n",
    "\n",
    "    return tagged\n",
    "\n",
    "tagged_documents = tag_documents_with_keywords(documents, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert Document objects to dicts\n",
    "doc_dicts = [doc.dict() for doc in tagged_documents]\n",
    "\n",
    "with open(\"tagged_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(doc_dicts, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166513b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tagged documents from file\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "with open(\"tagged_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tagged_documents = [Document(**doc) for doc in json.load(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f586616",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39882d72",
   "metadata": {},
   "source": [
    "Fuzzy-match query to keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db127464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "def find_keywords_in_prompt(prompt, keywords, threshold=80):\n",
    "    \"\"\"\n",
    "    Returns all keywords that appear in the prompt using fuzzy matching.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "        keywords (list): List of keywords to match.\n",
    "        threshold (int): Fuzzy match threshold (0-100).\n",
    "        \n",
    "    Returns:\n",
    "        list: Matched keywords.\n",
    "    \"\"\"\n",
    "    prompt_lower = prompt.lower()\n",
    "    matched = []\n",
    "    for kw in keywords:\n",
    "        kw_lower = kw.lower()\n",
    "        # Use partial_ratio for substring-like matching\n",
    "        if fuzz.partial_ratio(kw_lower, prompt_lower) >= threshold:\n",
    "            matched.append(kw)\n",
    "    return matched\n",
    "\n",
    "# Example usage:\n",
    "# keywords = [\"fever\", \"skin lesions\", \"swollen lymph nodes\"]\n",
    "# prompt = \"The patient presents with fever and swollen nodes.\"\n",
    "# print(find_keywords_in_prompt(prompt, keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51dd2f1",
   "metadata": {},
   "source": [
    "GPT to match query to keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "class KeywordsOutput(BaseModel):\n",
    "    keywords: List[str] = Field(description=\"List of relevant keywords extracted from the query\")\n",
    "\n",
    "def extract_keywords_with_gpt(query: str, known_keywords: List[str]) -> List[str]:\n",
    "    parser = PydanticOutputParser(pydantic_object=KeywordsOutput)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "You are helping identify relevant medical concepts. \n",
    "Given this query: \"{query}\"\n",
    "\n",
    "Select the most relevant keywords from this list:\n",
    "{keyword_list}\n",
    "\n",
    "Return the matching keywords as a JSON object with a single key \"keywords\" whose value is a list of strings.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "        input_variables=[\"query\", \"keyword_list\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(\n",
    "        llm=ChatOpenAI(temperature=0, model=\"gpt-4o\"),\n",
    "        prompt=prompt,\n",
    "        output_parser=parser,\n",
    "    )\n",
    "\n",
    "    output = chain.run(query=query, keyword_list=\", \".join(known_keywords))\n",
    "\n",
    "    # output is a list of strings, not a KeywordsOutput instance\n",
    "    return output.keywords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fdb67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_keywords = extract_keywords_with_gpt(query = \"child presenting with lesions\", known_keywords = keywords)\n",
    "# print(\"Matched Keywords:\", matched_keywords)\n",
    "type(matched_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4c9bc",
   "metadata": {},
   "source": [
    "Hybrid search using matched keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_with_query_keywords(query, vectorstore, documents, keyword_list, top_k=5):\n",
    "    # Step 1: Semantic search\n",
    "    semantic_hits = vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "    # Step 2: Use GPT to extract keywords from the query\n",
    "    matched_keywords = extract_keywords_with_gpt(query, keyword_list)\n",
    "\n",
    "    # Step 3: Filter docs whose metadata has any of those keywords\n",
    "    keyword_hits = [\n",
    "        doc for doc in documents\n",
    "        if any(\n",
    "            normalize_kw(kw1) == normalize_kw(kw2)\n",
    "            for kw1 in doc.metadata.get(\"matched_keywords\", [])\n",
    "            for kw2 in matched_keywords\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for kw in matched_keywords:\n",
    "        print(f\"Matched keyword: {kw}\")\n",
    "\n",
    "    # print metadata of keyword_hits\n",
    "    for doc in keyword_hits:\n",
    "        print(doc.metadata.get(\"disease_name\"))\n",
    "        print(doc.metadata.get(\"matched_keywords\"))\n",
    "        print(doc.page_content)\n",
    "\n",
    "    # Step 4: Merge by unique content\n",
    "    merged = {doc.page_content: doc for doc in semantic_hits + keyword_hits}\n",
    "    return list(merged.values()), matched_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# `documents` is the list of LangChain Document objects from before\n",
    "vectorstore = FAISS.from_documents(tagged_documents, embedding_model)\n",
    "\n",
    "vectorstore.save_local(\"disease_vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffa9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Startup:\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "vectorstore = FAISS.load_local(\"disease_vectorstore\", OpenAIEmbeddings(),allow_dangerous_deserialization=True)\n",
    "\n",
    "# Query time:\n",
    "query = \"child presenting with lesions\"\n",
    "results, matched = hybrid_search_with_query_keywords(query, vectorstore, tagged_documents, keywords)\n",
    "\n",
    "# print(\"Matched keywords:\", matched)\n",
    "# for doc in results:\n",
    "#     print(\"---\")\n",
    "#     print(doc.metadata.get(\"disease_name\"))\n",
    "#     print(doc.metadata.get(\"matched_keywords\"))\n",
    "#     print(doc.page_content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc=tagged_documents[0].metadata.get(\"matched_keywords\")\n",
    "doc\n",
    "# matched_keywords\n",
    "# doc in matched_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a99f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
